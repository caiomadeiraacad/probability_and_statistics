{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possível Solução para Lab I (LUCAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do *dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.5, 0.1], [0.2, 0.6]])\n",
    "print(X.shape)\n",
    "Y = np.array([[0.7], [0.8]])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição dos Parâmetros Treináveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "theta1 = np.array([[0.5, 0.2], \n",
    "                   [0.6, -0.1], \n",
    "                   [-0.4, -0.3]])\n",
    "print(theta1.shape)\n",
    "\n",
    "theta2 = np.array([[0.7, -0.1, 0.2]])\n",
    "print(theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "Nosso objetivo aqui é calcular o valor da saída da rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^{(1)} = X\\cdot\\theta^{(1)^\\top}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[ 0.27  0.29 -0.23]\n",
      " [ 0.22  0.06 -0.26]]\n"
     ]
    }
   ],
   "source": [
    "z1 = np.matmul(X, theta1.T)\n",
    "print(z1.shape)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$sigmoid(z)=\\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{(1)} = sigmoid(z^{(1)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = sigmoid(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^{(2)} = a^{(1)}\\cdot\\theta^{(2)^\\top}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "[[0.42831585]\n",
      " [0.42391866]]\n"
     ]
    }
   ],
   "source": [
    "z2 = np.matmul(a1, theta2.T)\n",
    "print(z2.shape)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{(2)} = sigmoid(z^{(2)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "[[0.60547144]\n",
      " [0.60442057]]\n"
     ]
    }
   ],
   "source": [
    "a2 = sigmoid(z2)\n",
    "print(a2.shape)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Nosso objetivo aqui é computar quão diferente a saída da rede está em relação ao valor da variável alvo $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$loss = \\frac{1}{2}(y-\\hat{y})^T(y-\\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02359348]]\n"
     ]
    }
   ],
   "source": [
    "loss = 0.5*np.matmul((Y-a2).T,(Y-a2))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "Nosso objetivo aqui é computar os gradientes da função de custo em relação aos parâmetros treináveis, $\\frac{\\partial J}{\\partial \\theta_1}$ e $\\frac{\\partial J}{\\partial \\theta_2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta^{(2)} = \\frac{\\partial J}{\\partial a_2}\\frac{\\partial a2}{\\partial z_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02258058],\n",
       "       [-0.04676233]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta2 = (a2 - Y) * a2 * (1 - a2)\n",
    "delta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta^{(1)} = \\frac{\\partial J}{\\partial a_2}\\frac{\\partial a2}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a1}{\\partial z_1} = \\delta^{(2)}\\theta^{(2)}\\odot a'^{(1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00388045,  0.00055281, -0.00111423],\n",
       "       [-0.00808518,  0.00116801, -0.00229904]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta1 = np.matmul(delta2, theta2) * a1 * (1 - a1)\n",
    "delta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J}{\\partial \\theta_2} = \\delta^{(2)^\\top} a^{(1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "[[-0.03874806 -0.03699839 -0.03035622]]\n"
     ]
    }
   ],
   "source": [
    "dJdtheta2 = np.matmul(delta2.T, a1)\n",
    "print(dJdtheta2.shape)\n",
    "print(dJdtheta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J}{\\partial \\theta_1} = \\delta^{(1)^\\top} X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[-0.00355726 -0.00523915]\n",
      " [ 0.00051001  0.00075609]\n",
      " [-0.00101692 -0.00149085]]\n"
     ]
    }
   ],
   "source": [
    "dJdtheta1 = np.matmul(delta1.T, X)\n",
    "print(dJdtheta1.shape)\n",
    "print(dJdtheta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Vamos usar os gradientes calculados no *backward pass* para atualizar os parâmetros treináveis.\n",
    "\n",
    "$\\theta = \\theta -\\eta\\frac{\\partial J}{\\partial \\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70387481 -0.09630016  0.20303562]]\n",
      "[[ 0.50035573  0.20052392]\n",
      " [ 0.599949   -0.10007561]\n",
      " [-0.39989831 -0.29985092]]\n"
     ]
    }
   ],
   "source": [
    "theta2 = theta2 - 0.1 * dJdtheta2\n",
    "theta1 = theta1 - 0.1 * dJdtheta1\n",
    "print(theta2)\n",
    "print(theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (minha versão)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "theta_um = np.array([[0.5, 0.2],\n",
    "                     [0.6, -0.1], \n",
    "                     [-0.4, -0.3]])\n",
    "print(theta_um.shape)\n",
    "\n",
    "theta_dois = np.array([[0.7, -0.1, 0.2]])\n",
    "print(theta_dois.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.5, 0.1], [0.2, 0.6]])\n",
    "print(X.shape)\n",
    "Y = np.array([[0.6], \n",
    "              [0.8]])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/ (1 + np.exp(-z))\n",
    "sigmoid(0) # tem que ser 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27,  0.29, -0.23],\n",
       "       [ 0.22,  0.06, -0.26]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = np.matmul(X, theta_um.T)\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5670929 , 0.57199613, 0.44275215],\n",
       "       [0.55477924, 0.5149955 , 0.43536371]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = sigmoid(z1)\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42831585],\n",
       "       [0.42391866]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 =np.matmul(a1, theta_dois.T)\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60547144],\n",
       "       [0.60442057]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = sigmoid(z2)\n",
    "a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01914063]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r =  Y - a2\n",
    "loss =np.matmul(r.T, r)/2\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00130699],\n",
       "       [-0.04676233]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dJda2 == derivada de J em relação a A2\n",
    "dJda2 = a2 - Y\n",
    "da2dz2 = a2 * (1 - a2)\n",
    "\n",
    "delta_dois = dJda2 * da2dz2\n",
    "delta_dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.24605550e-04, -3.19973721e-05,  6.44930033e-05],\n",
       "       [-8.08518134e-03,  1.16800666e-03, -2.29904316e-03]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da1dz1 = a1 * (1- a1)\n",
    "delta_um = np.matmul(delta_dois, theta_dois) * da1dz1\n",
    "\n",
    "delta_um"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00150473, -0.00482865],\n",
       "       [ 0.0002176 ,  0.0006976 ],\n",
       "       [-0.00042756, -0.00137298]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "djdteta2 = np.matmul(delta_dois.T, a1)\n",
    "djdteta2\n",
    "\n",
    "djdteta1 = np.matmul(delta_um.T, X)\n",
    "djdteta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50015047  0.20048286]\n",
      " [ 0.59997824 -0.10006976]\n",
      " [-0.39995724 -0.2998627 ]] (3, 2)\n",
      "[[ 0.70252016 -0.09766652  0.20197799]] (1, 3)\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "teta1 = theta_um - eta * djdteta1\n",
    "teta2 = theta_dois - eta * djdteta2\n",
    "\n",
    "print(teta1, teta1.shape)\n",
    "print(teta2, teta2. shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00130699]\n",
      " [-0.04676233]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m δ_l\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#  da1dz1 = a1 * (1 - a1)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#  δ_1 = np.matmul(δ_2, θ_2) * da1dz1\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m δ_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdelta_l\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_dois\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdelta_dois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mδ_1: \u001b[39m\u001b[38;5;124m\"\u001b[39m, delta_um)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(delta_i, a_previous):\n",
      "Cell \u001b[0;32mIn[47], line 12\u001b[0m, in \u001b[0;36mdelta_l\u001b[0;34m(delta_l_plus_one, theta_l_plus_one, al)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelta_l\u001b[39m(delta_l_plus_one, theta_l_plus_one, al):\n\u001b[1;32m     11\u001b[0m   daldzl \u001b[38;5;241m=\u001b[39m al \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m al)\n\u001b[0;32m---> 12\u001b[0m   δ_l \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_l_plus_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_l_plus_one\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m daldzl\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m δ_l\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)"
     ]
    }
   ],
   "source": [
    "def delta_L(aL, Y):\n",
    "  dj_daL = aL - \n",
    "  daLdzL = aL * (1 - aL)\n",
    "  δ_L = dj_daL * daLdzL\n",
    "  return δ_L\n",
    "\n",
    "δ_2 = delta_L(a2, Y)\n",
    "print(δ_2)\n",
    "\n",
    "def delta_l(delta_l_plus_one, theta_l_plus_one, al):\n",
    "  daldzl = al * (1 - al)\n",
    "  δ_l = np.matmul(delta_l_plus_one, theta_l_plus_one) * daldzl\n",
    "  return δ_l\n",
    "\n",
    "#  da1dz1 = a1 * (1 - a1)\n",
    "#  δ_1 = np.matmul(δ_2, θ_2) * da1dz1\n",
    "δ_1 = delta_l(δ_2, θ_2, a1)\n",
    "print(\"δ_1: \", δ_1)\n",
    "\n",
    "def gradient(delta_i, a_previous):\n",
    "  return np.matmul(delta_i.T, a_previous) \n",
    "\n",
    "#  djdθ2 = np.matmul(δ_2.T, a1)\n",
    "djdθ2 = gradient(δ_2, a1)\n",
    "print(\"djdθ2: \", djdθ2)\n",
    "\n",
    "#  djdθ1 = np.matmul(δ_1.T, X)\n",
    "djdθ1 = gradient(δ_1, X)\n",
    "print(\"djdθ1: \", djdθ1)\n",
    "\n",
    "def optimize(old_theta, gradient, eta=0.1):\n",
    "  return old_theta - eta * gradient\n",
    "\n",
    "# θ = θ - η(∂J / ∂θ)\n",
    "#  η = 0.1\n",
    "#  θ1_new = θ_1 - η * djdθ1\n",
    "θ1_new = optimize(θ_1, djdθ1)\n",
    "print(\"θ1_new: \", θ1_new, θ1_new.shape)\n",
    "\n",
    "#  θ2_new = θ_2 - η * djdθ2\n",
    "θ2_new = optimize(θ_2, djdθ2)\n",
    "print(\"θ2_new: \", θ2_new, θ2_new.shape)\n",
    "\n",
    "epochs = 1000\n",
    "η = 0.1\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    z1 = np.matmul(X, θ_1.T)\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.matmul(a1, θ_2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    r = y - a2\n",
    "    loss = np.matmul(r.T, r) / 2\n",
    "    losses.append(loss[0, 0])\n",
    "\n",
    "    δ_2 = delta_L(a2, y)\n",
    "    δ_1 = delta_l(δ_2, θ_2, a1)\n",
    "    \n",
    "    djdθ2 = gradient(δ_2, a1)\n",
    "    djdθ1 = gradient(δ_1, X)\n",
    "    \n",
    "    θ_1 = optimize(θ_1, djdθ1, η)\n",
    "    θ_2 = optimize(θ_2, djdθ2, η)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Época {epoch + 1}: Loss = {loss[0, 0]:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Época', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Loss x Época de Treinamento', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(1, epochs)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLoss inicial: {losses[0]:.6f}\")\n",
    "print(f\"Loss final: {losses[-1]:.6f}\")\n",
    "print(f\"Redução da loss: {(losses[0] - losses[-1]) / losses[0] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulas\n",
    "---\n",
    "## Forward Pass \n",
    "---\n",
    "$z^{(1)} = X\\cdot\\theta^{(1)^\\top}$\n",
    "---\n",
    "$a^{(1)} = sigmoid(z^{(1)})$\n",
    "---\n",
    "$z^{(2)} = a^{(1)}\\cdot\\theta^{(2)^\\top}$\n",
    "---\n",
    "$a^{(2)} = sigmoid(z^{(2)})$\n",
    "---\n",
    "## Função de Custo\n",
    "$loss = \\frac{1}{2}(y-\\hat{y})^T(y-\\hat{y})$\n",
    "---\n",
    "## Backward Pass:\n",
    "$\\delta^{(2)} = \\frac{\\partial J}{\\partial a_2}\\frac{\\partial a2}{\\partial z_2}$\n",
    "---\n",
    "$\\delta^{(1)} = \\frac{\\partial J}{\\partial a_2}\\frac{\\partial a2}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a1}{\\partial z_1} = \\delta^{(2)}\\theta^{(2)}\\odot a'^{(1)}$\n",
    "---\n",
    "$\\frac{\\partial J}{\\partial \\theta_2} = \\delta^{(2)^\\top} a^{(1)}$\n",
    "---\n",
    "$\\frac{\\partial J}{\\partial \\theta_1} = \\delta^{(1)^\\top} X$\n",
    "---\n",
    "## Otimizador\n",
    "---\n",
    "$\\theta = \\theta -\\eta\\frac{\\partial J}{\\partial \\theta}$\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
